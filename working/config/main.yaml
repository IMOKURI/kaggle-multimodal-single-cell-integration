defaults:
  - _self_

hydra:
  run:
    dir: ../output/${now:%Y-%m-%d_%H-%M-%S}
  job:
    chdir: False
  job_logging:
    formatters:
      simple:
        format: "%(asctime)s [%(levelname)s][%(module)s] %(message)s"

wandb:
  enabled: True
  entity: imokuri
  project: msci  # Multimodal Single-Cell Integration
  dir: ../cache
  group: ${global_params.method}

settings:
  print_freq: 100
  # gpus: "0,1"

  dirs:
    working: ..
    input: ${settings.dirs.working}/input
    feature: ${settings.dirs.working}/feature
    preprocess: ${settings.dirs.working}/preprocess
    # train_image: ${settings.dirs.input}/train/
    # test_image: ${settings.dirs.input}/test/
    # other_image: ${settings.dirs.input}/other/

  inputs:
    # - evaluation_ids.csv
    # - metadata.csv
    # - sample_submission.csv

    - test_cite_inputs.h5
    - train_cite_inputs.h5
    - train_cite_targets.h5

    - test_multi_inputs.h5
    - train_multi_inputs.h5
    - train_multi_targets.h5

    # - train.csv
    # - test.csv
    # - sample_submission.csv

  preprocesses:
    - test_cite_inputs.pickle
    - train_cite_inputs.pickle
    - train_cite_targets.pickle

    - test_multi_inputs.pickle
    - train_multi_inputs.pickle
    - train_multi_targets.pickle

  debug: False
  n_debug_data: 0

  amp: True
  multi_gpu: True

  skip_training: False
  skip_inference: False

  label_name: label
  n_class: 1
  scoring: pearson

# These parameters are main tuning parameters and recorded to wandb.
params:
  seed: ${global_params.seed}
  data: ${global_params.data}

global_params:
  seed: 440
  method: tabnet
  data: cite  # cite, multi

preprocess_params:
  methods:
    - pca
    # - ivis
  pca_n_components_cite: 240
  pca_n_components_multi: 40
  ivis_n_components_cite: 240
  ivis_n_components_multi: 4

cv_params:
  n_fold: 5
  n_validation: 1
  fold: kfold
  # group_name: patient_id
  # time_name: ""

training_params:
  epoch: 10
  es_patience: 0
  batch_size: 24
  gradient_acc_step: 1
  max_grad_norm: 1000
  # feature_set:
  #   - "f000" # f000_open_close
  criterion: CrossEntropyLoss
  optimizer: Adam
  scheduler: CosineAnnealingWarmupRestarts
  lr: 2e-2
  min_lr: 1e-5
  weight_decay: 1e-5  # AdamW: 0.01, others: 0
  # label_smoothing: 1e-6

model_params:
  dataset: base
  augmentation: light
  model: base
  model_input: 512
  model_name: tf_efficientnet_b4_ns
  # dropout: 0.0

inference_params:
  pretrained: []
  # pretrained:
  #   - dir: ""
  #     model: ""
  #     name: ""
